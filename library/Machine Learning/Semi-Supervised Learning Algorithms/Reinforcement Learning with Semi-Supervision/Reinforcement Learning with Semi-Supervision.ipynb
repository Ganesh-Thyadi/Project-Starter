{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOvUfOyWUhpG"
      },
      "source": [
        "## **Reinforcement Learning with Semi-Supervision**\n",
        "\n",
        "Reinforcement Learning with Semi-Supervision combines traditional RL with semi-supervised learning techniques to enhance agent learning with limited labeled data. In this setup, the agent learns from its interactions with the environment, updating its policy (Q-values) over time. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1TWE0LxU_-E"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z7qDmaLvVEUv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import deque\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3qgM8luVJcx"
      },
      "source": [
        "**Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N80pFaOVKtQ"
      },
      "outputs": [],
      "source": [
        "# Initialize a Gym environment\n",
        "env = gym.make('CartPole-v1')  # Example environment for RL\n",
        "\n",
        "# Define the number of episodes and maximum steps per episode\n",
        "n_episodes = 1000\n",
        "max_timesteps = 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qToSUcMJVPy8"
      },
      "source": [
        "**Minimal Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "jtdvWBDDVQ0m",
        "outputId": "040f53b9-965a-4d1e-ff51-490fa4fc544e"
      },
      "outputs": [],
      "source": [
        "# Normalize states if needed (optional)\n",
        "def preprocess_state(state):\n",
        "    return np.array(state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0a24kpEVfIZ"
      },
      "source": [
        "**Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGdDRzYRViZE"
      },
      "outputs": [],
      "source": [
        "# Define a simple Q-learning agent with semi-supervised learning\n",
        "class QLearningAgent:\n",
        "    def __init__(self, n_actions, gamma=0.99, alpha=0.1, epsilon=0.1):\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = {}\n",
        "\n",
        "    def get_q(self, state, action):\n",
        "        return self.Q.get((tuple(state), action), 0)\n",
        "\n",
        "    def update_q(self, state, action, reward, next_state, done):\n",
        "        best_next_action = np.argmax([self.get_q(next_state, a) for a in range(self.n_actions)])\n",
        "        td_target = reward + (self.gamma * self.get_q(next_state, best_next_action) * (1 - done))\n",
        "        td_error = td_target - self.get_q(state, action)\n",
        "        self.Q[(tuple(state), action)] = self.get_q(state, action) + self.alpha * td_error\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return np.random.choice(self.n_actions)\n",
        "        else:\n",
        "            q_values = [self.get_q(state, action) for action in range(self.n_actions)]\n",
        "            return np.argmax(q_values)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5BfgpGVln2"
      },
      "source": [
        "**Semi-Supervised Learning Step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcjFFa-bVpkk"
      },
      "outputs": [],
      "source": [
        "# Initialize the LabelSpreading model for semi-supervised learning\n",
        "label_spread_model = LabelSpreading(kernel='rbf', gamma=20)\n",
        "\n",
        "# Simulate pseudo-labeling based on the state and rewards\n",
        "def apply_semi_supervised_learning(state_action_pairs, rewards):\n",
        "    pseudo_labels = np.array([1 if r > 0 else 0 for r in rewards])  # 1 for positive rewards, 0 for negative\n",
        "    label_spread_model.fit(state_action_pairs, pseudo_labels)\n",
        "    return label_spread_model.predict(state_action_pairs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lJ3LJNlVuHt"
      },
      "source": [
        "**Training Loop with Reinforcement Learning and Semi-Supervision**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARqNM8R3Vynf"
      },
      "outputs": [],
      "source": [
        "# Initialize the Q-learning agent and training variables\n",
        "agent = QLearningAgent(n_actions=env.action_space.n)\n",
        "\n",
        "# Track states and rewards for semi-supervised learning\n",
        "state_action_pairs = []\n",
        "rewards = []\n",
        "\n",
        "# Training loop\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    for timestep in range(max_timesteps):\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Store state-action pairs and rewards\n",
        "        state_action_pairs.append(np.concatenate([state, [action]]))\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Update the Q-table with the reward\n",
        "        agent.update_q(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Apply semi-supervised learning (pseudo-labeling) periodically\n",
        "    if episode % 10 == 0:  # Apply semi-supervised learning every 10 episodes\n",
        "        pseudo_labels = apply_semi_supervised_learning(np.array(state_action_pairs), np.array(rewards))\n",
        "        print(f\"Episode {episode}, Pseudo-labeling applied.\")\n",
        "\n",
        "    print(f\"Episode {episode}, Total reward: {total_reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7wtaSt5V237"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEn_guUUV6V7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the agent after training\n",
        "total_rewards = 0\n",
        "for _ in range(10):  # Run 10 test episodes\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.select_action(state)  # Select the action based on the learned Q-values\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_rewards += reward\n",
        "\n",
        "print(f\"Average Reward over 10 test episodes: {total_rewards / 10}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UScDydSmWVaq"
      },
      "source": [
        "**Visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02w0svyrWXke"
      },
      "outputs": [],
      "source": [
        "# Visualizing the agent's performance over time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming total_rewards is tracked over episodes\n",
        "plt.plot(range(n_episodes), total_rewards)\n",
        "plt.title(\"Agent's Performance over Episodes\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
