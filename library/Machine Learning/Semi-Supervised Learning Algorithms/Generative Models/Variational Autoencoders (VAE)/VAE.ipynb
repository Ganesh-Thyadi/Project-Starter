{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOvUfOyWUhpG"
   },
   "source": [
    "## **Semi-Supervised Variational Autoencoders (VAE)**\n",
    "\n",
    "In semi-supervised learning, VAE can leverage the large amounts of unlabeled data while still benefiting from the small set of labeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1TWE0LxU_-E"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z7qDmaLvVEUv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3qgM8luVJcx"
   },
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N80pFaOVKtQ"
   },
   "outputs": [],
   "source": [
    "# Generating synthetic data (just for illustration)\n",
    "X = np.random.randn(1000, 20)\n",
    "y = np.random.randint(0, 2, size=(1000,))\n",
    "\n",
    "# Simulating unlabeled data (assigning some labels as -1)\n",
    "y[::5] = -1  # Assigning -1 (unlabeled) to every 5th sample\n",
    "\n",
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qToSUcMJVPy8"
   },
   "source": [
    "**Minimal Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "collapsed": true,
    "id": "jtdvWBDDVQ0m",
    "outputId": "040f53b9-965a-4d1e-ff51-490fa4fc544e"
   },
   "outputs": [],
   "source": [
    "# Normalize the data (if needed)\n",
    "# No significant preprocessing for this synthetic data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0a24kpEVfIZ"
   },
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGdDRzYRViZE"
   },
   "outputs": [],
   "source": [
    "# Define a simple VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2_mean = nn.Linear(512, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(512, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim, 512)\n",
    "        self.fc4 = nn.Linear(512, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2_mean(h1), self.fc2_logvar(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Initialize VAE\n",
    "vae = VAE(input_dim=20, latent_dim=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS5BfgpGVln2"
   },
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcjFFa-bVpkk"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.BCELoss(reduction='sum')(recon_x, x)\n",
    "    # Regularization term\n",
    "    # KL Divergence between Q(z|x) and P(z)\n",
    "    # (not directly using unlabeled data in this implementation, but can modify for semi-supervised)\n",
    "    # For simplicity, we'll ignore the exact loss implementation for the semi-supervised aspect here\n",
    "    return BCE\n",
    "\n",
    "# Train the model (simplified for illustration)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lJ3LJNlVuHt"
   },
   "source": [
    "**Performance Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARqNM8R3Vynf"
   },
   "outputs": [],
   "source": [
    "# The performance for VAEs is typically evaluated based on the reconstruction error and KL divergence.\n",
    "# We can calculate these using the loss function defined earlier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7wtaSt5V237"
   },
   "source": [
    "**Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEn_guUUV6V7"
   },
   "outputs": [],
   "source": [
    "# Visualize latent space for the trained VAE model\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    sample = torch.Tensor(X_test)\n",
    "    reconstructed, _, _ = vae(sample)\n",
    "    \n",
    "    # Plot the original vs reconstructed data\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(X_test[0, :], label='Original')\n",
    "    plt.plot(reconstructed[0, :].numpy(), label='Reconstructed')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
