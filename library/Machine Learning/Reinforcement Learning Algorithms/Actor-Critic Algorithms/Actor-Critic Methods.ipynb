{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOvUfOyWUhpG"
   },
   "source": [
    "## **Actor-Critic Methods**\n",
    "\n",
    "Actor-Critic methods combine the advantages of value-based and policy-based methods. The actor learns the policy (which action to take), and the critic evaluates the action by estimating the value function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1TWE0LxU_-E"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z7qDmaLvVEUv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3qgM8luVJcx"
   },
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N80pFaOVKtQ"
   },
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the actor and critic networks\n",
    "def create_actor_network():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(24, activation='relu', input_shape=env.observation_space.shape),\n",
    "        layers.Dense(env.action_space.n, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_critic_network():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(24, activation='relu', input_shape=env.observation_space.shape),\n",
    "        layers.Dense(1)  # Value prediction (state value)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0a24kpEVfIZ"
   },
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGdDRzYRViZE"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99  # Discount factor\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize models\n",
    "actor_model = create_actor_network()\n",
    "critic_model = create_critic_network()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Define the actor-critic training loop\n",
    "def train_step(state, action, reward, next_state, done):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = actor_model(state)\n",
    "        action_prob = logits[0, action]\n",
    "        value = critic_model(state)\n",
    "        next_value = critic_model(next_state)\n",
    "        td_target = reward + gamma * next_value * (1 - done)\n",
    "        td_error = td_target - value\n",
    "\n",
    "        # Actor loss (policy gradient)\n",
    "        actor_loss = -tf.math.log(action_prob) * td_error\n",
    "        # Critic loss (value prediction)\n",
    "        critic_loss = td_error ** 2\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "    grads = tape.gradient(total_loss, actor_model.trainable_variables + critic_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, actor_model.trainable_variables + critic_model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS5BfgpGVln2"
   },
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcjFFa-bVpkk"
   },
   "outputs": [],
   "source": [
    "def actor_critic(env, n_episodes=1000):\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            action_probs = actor_model(state)\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=action_probs[0].numpy())\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            train_step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "actor_critic(env)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
