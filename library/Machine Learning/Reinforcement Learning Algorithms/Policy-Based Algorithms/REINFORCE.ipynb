{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOvUfOyWUhpG"
   },
   "source": [
    "## **Policy-Based Methods (REINFORCE)**\n",
    "\n",
    "In policy-based methods, the agent directly learns a policy function that maps states to actions, rather than learning a value function. REINFORCE is a Monte Carlo policy gradient method that updates the policy by using the rewards received in an episode to adjust the likelihood of actions that led to higher rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1TWE0LxU_-E"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z7qDmaLvVEUv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3qgM8luVJcx"
   },
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N80pFaOVKtQ"
   },
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Define the policy network\n",
    "def create_policy_network():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(24, activation='relu', input_shape=env.observation_space.shape),\n",
    "        layers.Dense(24, activation='relu'),\n",
    "        layers.Dense(env.action_space.n, activation='softmax')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0a24kpEVfIZ"
   },
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGdDRzYRViZE"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99  # Discount factor\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Define the policy gradient agent\n",
    "policy_model = create_policy_network()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "def compute_loss(log_probs, rewards, gamma=gamma):\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in rewards[::-1]:\n",
    "        cumulative_reward = reward + cumulative_reward * gamma\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    discounted_rewards = np.array(discounted_rewards)\n",
    "\n",
    "    log_probs = np.array(log_probs)\n",
    "    loss = -np.sum(log_probs * discounted_rewards)\n",
    "    return loss\n",
    "\n",
    "def train_step(state, action, reward):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = policy_model(state)\n",
    "        log_probs = tf.math.log(logits[0, action])\n",
    "        loss = compute_loss(log_probs, reward)\n",
    "    grads = tape.gradient(loss, policy_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, policy_model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS5BfgpGVln2"
   },
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcjFFa-bVpkk"
   },
   "outputs": [],
   "source": [
    "    def reinforce(env, n_episodes=1000):\n",
    "        for episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            rewards = []\n",
    "            actions = []\n",
    "            log_probs = []\n",
    "            while not done:\n",
    "                state = np.expand_dims(state, axis=0)  # Add batch dimension\n",
    "                action_probs = policy_model(state)\n",
    "                action = np.random.choice(np.arange(env.action_space.n), p=action_probs[0].numpy())\n",
    "                log_prob = np.log(action_probs[0][action])\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            train_step(np.array(actions), rewards)\n",
    "\n",
    "    reinforce(env)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
