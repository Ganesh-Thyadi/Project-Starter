{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOvUfOyWUhpG"
      },
      "source": [
        "## **Value-Based Methods (Q-Learning)**\n",
        "\n",
        "Value-based methods aim to learn a value function that estimates the long-term return of each state-action pair. Q-Learning is one of the most popular algorithms, where an agent updates a Q-table based on the actions it takes and the rewards it receives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1TWE0LxU_-E"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z7qDmaLvVEUv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3qgM8luVJcx"
      },
      "source": [
        "**Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N80pFaOVKtQ"
      },
      "outputs": [],
      "source": [
        "# Create an environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Initialize Q-table\n",
        "n_actions = env.action_space.n\n",
        "n_states = env.observation_space.shape[0]  # Number of state variables\n",
        "Q = np.zeros((n_states, n_actions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0a24kpEVfIZ"
      },
      "source": [
        "**Model Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGdDRzYRViZE"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "# Q-Learning Algorithm\n",
        "def q_learning(env, n_episodes=1000):\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()  # Explore\n",
        "            else:\n",
        "                action = np.argmax(Q[state])  # Exploit\n",
        "            \n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "            state = next_state\n",
        "\n",
        "q_learning(env)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5BfgpGVln2"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcjFFa-bVpkk"
      },
      "outputs": [],
      "source": [
        "# After training, use the learned Q-table for predictions\n",
        "state = env.reset()\n",
        "action = np.argmax(Q[state])  # Select the best action based on the learned Q-values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lJ3LJNlVuHt"
      },
      "source": [
        "**Performance Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARqNM8R3Vynf"
      },
      "outputs": [],
      "source": [
        "# Evaluate the agent's performance after training\n",
        "total_rewards = 0\n",
        "for _ in range(10):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = np.argmax(Q[state])  # Use the policy derived from the Q-table\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_rewards += reward\n",
        "print(f\"Average Reward: {total_rewards / 10}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
