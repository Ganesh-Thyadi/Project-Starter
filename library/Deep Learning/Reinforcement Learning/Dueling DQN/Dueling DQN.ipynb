{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dueling DQN**\n",
    "\n",
    "Extends DQN by splitting the Q-value into state-value and advantage-value components for better learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the Dueling DQN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value(x)\n",
    "        advantage = self.advantage(x)\n",
    "        return value + advantage - advantage.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize environment and parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "input_size = env.observation_space.shape[0]\n",
    "hidden_size = 128\n",
    "output_size = env.action_space.n\n",
    "model = DuelingDQN(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "memory = deque(maxlen=10000)\n",
    "target_update_frequency = 10\n",
    "steps_done = 0\n",
    "\n",
    "# Function to select action\n",
    "def select_action(state):\n",
    "    global epsilon\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = model(state_tensor)\n",
    "            return torch.argmax(q_values).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to optimize the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = random.sample(memory, batch_size)\n",
    "    batch = list(zip(*transitions))\n",
    "    states, actions, rewards, next_states, dones = batch\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.long)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "    q_values = model(states)\n",
    "    next_q_values = model(next_states)\n",
    "    next_q_value = next_q_values.max(1)[0]\n",
    "    expected_q_values = rewards + (gamma * next_q_value * (1 - dones))\n",
    "\n",
    "    loss = loss_fn(q_values.gather(1, actions.unsqueeze(1)).squeeze(1), expected_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for episode in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        optimize_model()\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    if episode % target_update_frequency == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
