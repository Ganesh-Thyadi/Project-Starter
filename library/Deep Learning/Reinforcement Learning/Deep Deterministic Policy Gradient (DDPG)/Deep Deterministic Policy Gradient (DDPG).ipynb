{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Deterministic Policy Gradient (DDPG)**\n",
    "\n",
    "An actor-critic algorithm for continuous action spaces that uses deterministic policies and experience reply.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Actor and Critic Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.max_action * self.model(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Networks and Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "actor = Actor(state_dim=3, action_dim=1, max_action=1)\n",
    "critic = Critic(state_dim=3, action_dim=1)\n",
    "actor_target = Actor(state_dim=3, action_dim=1, max_action=1)\n",
    "critic_target = Critic(state_dim=3, action_dim=1)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=1e-3)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Replay Buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.random.choice(self.buffer, batch_size, replace=False)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(state, dtype=torch.float32),\n",
    "            torch.tensor(action, dtype=torch.float32),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            torch.tensor(next_state, dtype=torch.float32),\n",
    "            torch.tensor(done, dtype=torch.float32)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    replay_buffer = ReplayBuffer(1000000)\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for t in range(1000000):\n",
    "        action = actor(torch.tensor(state, dtype=torch.float32))\n",
    "        next_state, reward, done, _ = env.step(action.detach().numpy())\n",
    "        replay_buffer.push(state, action.detach().numpy(), reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(replay_buffer.buffer) > 1000:\n",
    "            batch = replay_buffer.sample(100)\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, done_batch = batch\n",
    "\n",
    "            # Update Critic\n",
    "            target_q = reward_batch + (1 - done_batch) * 0.99 * critic_target(next_state_batch, actor_target(next_state_batch))\n",
    "            current_q = critic(state_batch, action_batch)\n",
    "            critic_loss = nn.MSELoss()(current_q, target_q)\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update Actor\n",
    "            actor_loss = -critic(state_batch, actor(state_batch)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Update Target Networks\n",
    "            for target_param, param in zip(actor_target.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(0.995 * target_param.data + 0.005 * param.data)\n",
    "            for target_param, param in zip(critic_target.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(0.995 * target_param.data + 0.005 * param.data)\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
