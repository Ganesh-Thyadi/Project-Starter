{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOvUfOyWUhpG"
   },
   "source": [
    "## **Additive Attention**\n",
    "\n",
    "Additive Attention, also known as Bahdanau attention, computes the attention weights based on a learned scoring function. The attention score is calculated as the sum of a query and a key, followed by a non-linear activation function (typically a tanh function), which is then passed through a feed-forward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1TWE0LxU_-E"
   },
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z7qDmaLvVEUv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3qgM8luVJcx"
   },
   "source": [
    "**Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N80pFaOVKtQ"
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(10, 20)  # Random data for input (10 sequences, 20 features)\n",
    "query = torch.randn(1, 20)  # A single query vector\n",
    "key = torch.randn(10, 20)  # Key for all sequences\n",
    "value = torch.randn(10, 20)  # Value for all sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0a24kpEVfIZ"
   },
   "source": [
    "**Additive Attention Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGdDRzYRViZE"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.energy_layer = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        query = self.query_layer(query).unsqueeze(0)\n",
    "        key = self.key_layer(key)\n",
    "        energy = torch.tanh(query + key)\n",
    "        attention_weights = torch.softmax(self.energy_layer(energy).squeeze(), dim=0)\n",
    "        weighted_sum = torch.sum(attention_weights.unsqueeze(1) * value, dim=0)\n",
    "        return weighted_sum, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gS5BfgpGVln2"
   },
   "source": [
    "**Instantiate and Apply Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcjFFa-bVpkk"
   },
   "outputs": [],
   "source": [
    "attention = AdditiveAttention(input_dim=20, hidden_dim=32)\n",
    "weighted_sum, attention_weights = attention(query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lJ3LJNlVuHt"
   },
   "source": [
    "**Display Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARqNM8R3Vynf"
   },
   "outputs": [],
   "source": [
    "print(\"Weighted Sum:\", weighted_sum)\n",
    "print(\"Attention Weights:\", attention_weights)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
