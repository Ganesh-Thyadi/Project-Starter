{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CBOW (Continuous Bag of Words)**\n",
    "\n",
    "Continuous Bag of Words\" (CBOW) is a neural network model used in Natural Language Processing (NLP) to learn word embeddings, which are vector representations of words that capture their semantic and syntactic relationships, by predicting a target word based on its surrounding context words within a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy gensim spacy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Text processing is an essential part of NLP.\",\n",
    "    \"Word embeddings capture semantic meaning of words.\",\n",
    "    \"Spacy and Gensim are popular NLP libraries.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words Representation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Representation:\n",
      " [[1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1]\n",
      " [0 1 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(documents)\n",
    "print(\"Bag of Words Representation:\\n\", X_bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF Representation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Representation:\n",
      " [[0.37380112 0.         0.         0.         0.         0.37380112\n",
      "  0.         0.37380112 0.         0.         0.28428538 0.28428538\n",
      "  0.37380112 0.         0.37380112 0.         0.         0.37380112\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.38988801 0.38988801 0.\n",
      "  0.         0.         0.         0.38988801 0.         0.29651988\n",
      "  0.         0.         0.         0.38988801 0.         0.\n",
      "  0.38988801 0.38988801]\n",
      " [0.         0.38988801 0.38988801 0.         0.         0.\n",
      "  0.38988801 0.         0.38988801 0.         0.29651988 0.\n",
      "  0.         0.38988801 0.         0.         0.38988801 0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "print(\"TF-IDF Representation:\\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Embedding for 'text':\n",
      " [ 0.00094564  0.0307732  -0.06812645 -0.01375465  0.07668581  0.0734641\n",
      " -0.03673297  0.02642702 -0.08317129  0.06205486]\n"
     ]
    }
   ],
   "source": [
    "sentences = [doc.lower().split() for doc in documents]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=10, window=5, min_count=1, workers=2)\n",
    "print(\"Word2Vec Embedding for 'text':\\n\", word2vec_model.wv['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spacy Word Embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy Word Embedding for 'text':\n",
      " [-0.3143282  -1.4313254   0.97116375  1.1657453  -0.78642035 -0.46428394\n",
      "  1.2019652   1.3568258  -0.2798018  -1.2564946   0.4840206  -0.5896061\n",
      " -0.06185064 -0.4479941  -0.5028136   0.49722886 -0.63204694 -0.22040193\n",
      "  0.02757305  0.4600879   0.28234607  0.46573967 -0.38013345 -0.14335965\n",
      "  0.8882024  -0.05255298 -0.17257833  0.57981324 -0.14689963  0.439454\n",
      " -0.19292976 -0.11136664  0.00469905 -0.86727947 -0.04117572 -0.6702051\n",
      " -0.7805647  -0.14242868 -0.11098792  1.1332091  -0.54676765  0.81636554\n",
      " -0.5482671   0.22478546  0.32625276  0.06537245 -0.3599271   0.68783486\n",
      "  0.5495992  -0.7473009   0.47752792  1.0430102   1.3414547  -0.71042055\n",
      "  0.54571635 -0.7700973  -0.12274662 -0.12977678  0.02895647 -0.37396997\n",
      " -0.93757653  0.5367556  -0.43098864 -1.2304637  -0.05791636 -1.0462031\n",
      "  1.0053499   0.53844917  0.7979429  -1.0634456   0.00697577  1.0017843\n",
      " -0.0815018  -0.98491645  0.39395735 -1.1745528  -0.41442788 -0.83574855\n",
      "  0.98226875 -0.6173926  -0.43172655 -0.557662   -0.26895693 -1.5820963\n",
      " -0.01766844  1.1230363   0.74521405  0.07253528  0.9395605   0.735974\n",
      "  0.13379613 -0.5312147   0.27690157  0.58946437  0.30796713 -0.30264866]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Text processing is an essential part of NLP.\")\n",
    "print(\"Spacy Word Embedding for 'text':\\n\", doc[0].vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
