{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Normalisation**\n",
    "\n",
    "Text normalization is the process of converting text into a standard format. It's a vital step in natural language processing (NLP) and is used before storing, processing, or analyzing text. Text normalization ensures that input is consistent, which helps improve the accuracy of NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in c:\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: click in c:\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python312\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\python312\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\python312\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\python312\\lib\\site-packages (from spacy) (2.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from spacy) (74.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python312\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ch.k.abhiram\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.17.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (6.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.4/12.8 MB 26.9 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.7/12.8 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.8/12.8 MB 10.1 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.4/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.6 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ch.K.Abhiram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ch.K.Abhiram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ch.K.Abhiram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ch.K.Abhiram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "### Step 2: Import Libraries\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "### Step 3: Load Required NLTK Data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Optional for better lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello! This is an example sentence for text normalization. We're testing tokenization, stopword removal, stemming, and lemmatization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenisation and Lemmatisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokens: ['Hello!', 'This is an example sentence for text normalization.', \"We're testing tokenization, stopword removal, stemming, and lemmatization.\"]\n",
      "Word Tokens: ['Hello', 'This', 'is', 'an', 'example', 'sentence', 'for', 'text', 'normalization', 'We', 're', 'testing', 'tokenization', 'stopword', 'removal', 'stemming', 'and', 'lemmatization']\n",
      "Lowercased Words: ['hello', 'this', 'is', 'an', 'example', 'sentence', 'for', 'text', 'normalization', 'we', 're', 'testing', 'tokenization', 'stopword', 'removal', 'stemming', 'and', 'lemmatization']\n",
      "Filtered Words: ['hello', 'example', 'sentence', 'text', 'normalization', 'testing', 'tokenization', 'stopword', 'removal', 'stemming', 'lemmatization']\n",
      "Stemmed Words: ['hello', 'exampl', 'sentenc', 'text', 'normal', 'test', 'token', 'stopword', 'remov', 'stem', 'lemmat']\n",
      "Lemmatized Words (WordNet): ['hello', 'example', 'sentence', 'text', 'normalization', 'testing', 'tokenization', 'stopword', 'removal', 'stemming', 'lemmatization']\n",
      "Lemmatized Words (spaCy): ['hello', 'example', 'sentence', 'text', 'normalization', 'testing', 'tokenization', 'stopword', 'removal', 'stem', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization using regex\n",
    "sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "print(\"Sentence Tokens:\", sentences)\n",
    "\n",
    "# Word Tokenization using regex\n",
    "words = re.findall(r'\\b\\w+\\b', text)\n",
    "print(\"Word Tokens:\", words)\n",
    "\n",
    "### Step 6: Lowercasing\n",
    "words_lower = [word.lower() for word in words]\n",
    "print(\"Lowercased Words:\", words_lower)\n",
    "\n",
    "### Step 7: Remove Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words_lower if word not in stop_words]\n",
    "print(\"Filtered Words:\", filtered_words)\n",
    "\n",
    "### Step 8: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "### Step 9: Lemmatization (WordNet + spaCy)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# WordNet Lemmatizer\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Lemmatized Words (WordNet):\", lemmatized_words)\n",
    "\n",
    "# spaCy Lemmatization\n",
    "doc = nlp(\" \".join(filtered_words))\n",
    "lemmatized_words_spacy = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatized Words (spaCy):\", lemmatized_words_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Normalized Text: hello example sentence text normalization testing tokenization stopword removal stem lemmatization\n"
     ]
    }
   ],
   "source": [
    "normalized_text = \" \".join(lemmatized_words_spacy)\n",
    "print(\"Final Normalized Text:\", normalized_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
